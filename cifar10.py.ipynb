{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Couldn't find checkpoint to restore from. Starting over.\n",
      "training on image #0\n",
      "training on image #5000\n",
      "training on image #10000\n",
      "training on image #15000\n",
      "training on image #20000\n",
      "training on image #25000\n",
      "training on image #30000\n",
      "training on image #35000\n",
      "training on image #40000\n",
      "training on image #45000\n",
      "training on image #0\n",
      "training on image #5000\n",
      "training on image #10000\n",
      "training on image #15000\n",
      "training on image #20000\n",
      "training on image #25000\n",
      "training on image #30000\n",
      "training on image #35000\n",
      "training on image #40000\n",
      "training on image #45000\n",
      "training on image #0\n",
      "training on image #5000\n",
      "training on image #10000\n",
      "training on image #15000\n",
      "training on image #20000\n",
      "training on image #25000\n",
      "training on image #30000\n",
      "training on image #35000\n",
      "training on image #40000\n",
      "training on image #45000\n",
      "training on image #0\n",
      "training on image #5000\n",
      "training on image #10000\n",
      "training on image #15000\n",
      "training on image #20000\n",
      "training on image #25000\n",
      "training on image #30000\n",
      "training on image #35000\n",
      "training on image #40000\n",
      "training on image #45000\n",
      "training on image #0\n",
      "training on image #5000\n",
      "training on image #10000\n",
      "training on image #15000\n",
      "training on image #20000\n",
      "training on image #25000\n",
      "training on image #30000\n",
      "training on image #35000\n",
      "training on image #40000\n",
      "training on image #45000\n",
      "training on image #0\n",
      "training on image #5000\n",
      "training on image #10000\n",
      "training on image #15000\n",
      "training on image #20000\n",
      "training on image #25000\n",
      "training on image #30000\n",
      "training on image #35000\n",
      "training on image #40000\n",
      "training on image #45000\n",
      "training on image #0\n",
      "training on image #5000\n",
      "training on image #10000\n",
      "training on image #15000\n",
      "training on image #20000\n",
      "training on image #25000\n",
      "training on image #30000\n",
      "training on image #35000\n",
      "training on image #40000\n",
      "training on image #45000\n",
      "training on image #0\n",
      "training on image #5000\n",
      "training on image #10000\n",
      "training on image #15000\n",
      "training on image #20000\n",
      "training on image #25000\n",
      "training on image #30000\n",
      "training on image #35000\n",
      "training on image #40000\n",
      "training on image #45000\n",
      "training on image #0\n",
      "training on image #5000\n",
      "training on image #10000\n",
      "training on image #15000\n",
      "training on image #20000\n",
      "training on image #25000\n",
      "training on image #30000\n",
      "training on image #35000\n",
      "training on image #40000\n",
      "training on image #45000\n",
      "training on image #0\n",
      "training on image #5000\n",
      "training on image #10000\n",
      "training on image #15000\n",
      "training on image #20000\n",
      "training on image #25000\n",
      "training on image #30000\n",
      "training on image #35000\n",
      "training on image #40000\n",
      "training on image #45000\n",
      "[0.456]\n",
      "[0.432]\n",
      "[0.448]\n",
      "[0.384]\n",
      "[0.528]\n",
      "[0.376]\n",
      "[0.488]\n",
      "[0.456]\n",
      "[0.504]\n",
      "[0.456]\n",
      "[0.432]\n",
      "[0.488]\n",
      "[0.432]\n",
      "[0.488]\n",
      "[0.552]\n",
      "[0.48]\n",
      "[0.472]\n",
      "[0.456]\n",
      "[0.488]\n",
      "[0.472]\n",
      "[0.368]\n",
      "[0.536]\n",
      "[0.384]\n",
      "[0.448]\n",
      "[0.496]\n",
      "[0.448]\n",
      "[0.464]\n",
      "[0.44]\n",
      "[0.416]\n",
      "[0.456]\n",
      "[0.472]\n",
      "[0.44]\n",
      "[0.408]\n",
      "[0.504]\n",
      "[0.464]\n",
      "[0.448]\n",
      "[0.48]\n",
      "[0.528]\n",
      "[0.424]\n",
      "[0.456]\n",
      "[0.536]\n",
      "[0.456]\n",
      "[0.432]\n",
      "[0.4]\n",
      "[0.512]\n",
      "[0.488]\n",
      "[0.416]\n",
      "[0.504]\n",
      "[0.408]\n",
      "[0.488]\n",
      "[0.512]\n",
      "[0.432]\n",
      "[0.408]\n",
      "[0.448]\n",
      "[0.392]\n",
      "[0.4]\n",
      "[0.52]\n",
      "[0.456]\n",
      "[0.488]\n",
      "[0.416]\n",
      "[0.488]\n",
      "[0.448]\n",
      "[0.48]\n",
      "[0.528]\n",
      "[0.464]\n",
      "[0.448]\n",
      "[0.416]\n",
      "[0.448]\n",
      "[0.456]\n",
      "[0.432]\n",
      "[0.472]\n",
      "[0.432]\n",
      "[0.408]\n",
      "[0.52]\n",
      "[0.456]\n",
      "[0.536]\n",
      "[0.512]\n",
      "[0.512]\n",
      "[0.32]\n"
     ]
    }
   ],
   "source": [
    "import models\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = pickle.load(fo, encoding='latin1')\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "def one_hot_vec(label):\n",
    "    vec = np.zeros(10)\n",
    "    vec[label] = 1\n",
    "    return vec\n",
    "\n",
    "def load_data():\n",
    "    x_all = []\n",
    "    y_all = []\n",
    "    for i in range (5):\n",
    "        d = unpickle(\"cifar-10-batches-py/data_batch_\" + str(i+1))\n",
    "        x_ = d['data']\n",
    "        y_ = d['labels']\n",
    "        x_all.append(x_)\n",
    "        y_all.append(y_)\n",
    "\n",
    "    d = unpickle('cifar-10-batches-py/test_batch')\n",
    "    x_all.append(d['data'])\n",
    "    y_all.append(d['labels'])\n",
    "\n",
    "    x = np.concatenate(x_all) / np.float32(255)\n",
    "    y = np.concatenate(y_all)\n",
    "    x = np.dstack((x[:, :1024], x[:, 1024:2048], x[:, 2048:]))\n",
    "    x = x.reshape((x.shape[0], 32, 32, 3))\n",
    "    \n",
    "    pixel_mean = np.mean(x[0:50000],axis=0)\n",
    "    x -= pixel_mean\n",
    "\n",
    "    y = map(one_hot_vec, y)\n",
    "    y = list(y)\n",
    "    X_train = x[0:50000,:,:,:]\n",
    "    Y_train = y[0:50000]\n",
    "    X_test = x[50000:,:,:,:]\n",
    "    Y_test = y[50000:]\n",
    "\n",
    "    return (X_train, Y_train, X_test, Y_test)\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = load_data()\n",
    "\n",
    "batch_size = 125\n",
    "\n",
    "X = tf.placeholder(\"float\", [batch_size, 32, 32, 3])\n",
    "Y = tf.placeholder(\"float\", [batch_size, 10])\n",
    "learning_rate = tf.placeholder(\"float\", [])\n",
    "\n",
    "# ResNet Models\n",
    "net = models.resnet(X, 20)\n",
    "# net = models.resnet(X, 32)\n",
    "# net = models.resnet(X, 44)\n",
    "# net = models.resnet(X, 56)\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(Y*tf.log(net))\n",
    "opt = tf.train.MomentumOptimizer(learning_rate, 0.9)\n",
    "train_op = opt.minimize(cross_entropy)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(net, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "checkpoint = tf.train.latest_checkpoint(\".\")\n",
    "if checkpoint:\n",
    "    print(\"Restoring from checkpoint\", checkpoint)\n",
    "    saver.restore(sess, checkpoint)\n",
    "else:\n",
    "    print(\"Couldn't find checkpoint to restore from. Starting over.\")\n",
    "\n",
    "for j in range (10):\n",
    "    for i in range (0, 50000, batch_size):\n",
    "        rate = 0.01\n",
    "        if j*50000+i/batch_size < 40000:\n",
    "            rate = 0.01\n",
    "        elif j*50000+i/batch_size < 80000:\n",
    "            rate = 0.001\n",
    "        else:\n",
    "            rate = 0.0001\n",
    "        feed_dict={\n",
    "            X: X_train[i:i + batch_size], \n",
    "            Y: Y_train[i:i + batch_size],\n",
    "            learning_rate: rate}\n",
    "        sess.run([train_op], feed_dict=feed_dict)\n",
    "        if i % 5000 == 0:\n",
    "            print(\"training on image #%d\" % i)\n",
    "saver.save(sess, os.getcwd()+\"/progress/model.ckpt\", global_step=i)\n",
    "\n",
    "for i in range (0, 10000, batch_size):\n",
    "    if i + batch_size < 10000:\n",
    "        acc = sess.run([accuracy],feed_dict={\n",
    "            X: X_test[i:i+batch_size],\n",
    "            Y: Y_test[i:i+batch_size]\n",
    "        })\n",
    "        accuracy_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "        print(acc)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
